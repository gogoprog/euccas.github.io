<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">

  <title><![CDATA[Category: System-design | euccas.github.io]]></title>
  <link href="http://euccas.github.io/categories/system-design/atom.xml" rel="self"/>
  <link href="http://euccas.github.io/"/>
  <updated>2018-02-13T17:39:32-08:00</updated>
  <id>http://euccas.github.io/</id>
  <author>
    <name><![CDATA[euccas]]></name>
    
  </author>
  <generator uri="http://octopress.org/">Octopress</generator>

  
  <entry>
    <title type="html"><![CDATA[Facebook Infrastructure: Streaming Video Engine (SVE)]]></title>
    <link href="http://euccas.github.io/blog/20170627/facebook-infrastructure-streaming-video-engine-sve.html"/>
    <updated>2017-06-27T22:16:02-07:00</updated>
    <id>http://euccas.github.io/blog/20170627/facebook-infrastructure-streaming-video-engine-sve</id>
    <content type="html"><![CDATA[<p>In last year’s <a href="https://developers.facebook.com/videos/?category=f8_2016"><strong>Facebook F8 conference</strong></a>, Sachin Kulkarni, who worked on Facebook’s Video Infrastructure, gave a talk (<a href="https://developers.facebook.com/videos/f8-2016/inside-look-at-facebook-media-infrastructure/">watch it here</a>) to introduce the design of Facebook’s <strong>Streaming Video Engine System (SVE)</strong>. I found this talk particularly interesting because it revealed, in a very well structured, concise yet informative way, how Facebook infrastructure team came up with a solution to build a video system solving user frustrations by reviewing the end-to-end process, and how such a design meet the goal of being <strong>fast</strong>, <strong>flexible</strong>, <strong>scalable</strong>, and <strong>efficient</strong>. After watching the presentation video for a few times, I thought it would be helpful to write down some notes here, for my own reviewing in the future, and for people who might be interested in Facebook’s media infrastructure.</p>

<p>Sharing on Facebook started from largely text, and quickly changed to be largely photos. Since 2014, more videos started to be posted and shared among users. The challenge was, building a video processing system is much harder than building a text or image processing system. Videos are greedy, they will consume all your resources: CPU, memory, disk, network, and anything else.</p>

<p>Before building the Streaming Video Engine system, the team started by reviewing Facebook’s existing video uploading and processing process, which was slow and not scalable. They found several problems need change or improvement:</p>

<!--more-->

<ul>
  <li>No unified clients</li>
  <li>Several disk reads and writes in the critical path</li>
  <li>Was doing serial processing throughout</li>
  <li>Read a video as one single big file, instead of splitting it up to chunks</li>
</ul>

<p>The new Streaming Video Engine (SVE) is expected to solve the aforementioned problems, and to meet the four design goals:</p>

<ul>
  <li>Fast: make users upload their videos super fast</li>
  <li>Flexible: usable for different Facebook products</li>
  <li>Scalable: everything at Facebook has to scale</li>
  <li>Efficient: storage efficiency, processing efficiency, and more importantly consume less bytes of users’ data plan</li>
</ul>

<p>These four design goals, in my opinion, are also the most common goals applicable to most engineering infrastructure systems.</p>

<p>Let’s take a deep dive to see how SVE was designed to meet these goals.</p>

<h1 id="fast">Fast</h1>

<ul>
  <li>First step is build a common library (for video uploading) that could be used for the clients cross platforms (Web, Mobile, Android, etc.). With the common library, optimizations on video uploading can be applied to all platforms.</li>
  <li>The uploading library has functions to split a video by GOP (Group of Pictures, a GOP roughly is a scene in the video) alignment. So any given video can be split to segments, which can have multiple GOPs.</li>
  <li>Uploading process starts as soon as the clients split a video into segaments. The <strong>client</strong> uploads one segment a time to the <strong>web server</strong>.</li>
  <li>Web server sends out segments to the <strong>preprocessor</strong>, which is a write-through cache.</li>
  <li>Proprocessor handles:
    <ul>
      <li>Normalize the video (segment) if it needs to</li>
      <li>Notify the <strong>scheduler</strong> that there are video segments available to be encoded</li>
      <li>Write the video (segment) to the <strong>original storage</strong></li>
      <li>Further split the video segment into GOPs</li>
    </ul>
  </li>
  <li>Scheduler will find workers to encode videos. Multiple works can be utilized and each worker will process one or multiple GOPs.</li>
  <li>Overlapped upload and encoding process: While proprocessor, scheduler and works are working, the uploading process is still ongoing. Clients continues splitting videos into segments and uploading to the web server.</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_00.png" width="600"></p>

<p>With this design, the process speedup reached 2.3x (small videos &lt; 3MB) ~ 9.3x (large videos &gt; 1G).</p>

<h1 id="flexible">Flexible</h1>

<ul>
  <li>The key insight that allows SVE to be flexible is, all the video processing pipelines can be represented as a DAG (Directed Acyclic Graph).</li>
  <li>Arbitrary dependencies can be added between the tasks in the video processing pipepline. The added tasks can be executed in parallel while the main pipeline tasks are running.</li>
  <li>SVE provides very simple API functions for the video pipeline (Ideally, you can add a video processing pipeline in your product in less than 10 lines of code).</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_01.png" width="600"></p>

<h1 id="scalable">Scalable</h1>

<ul>
  <li>SVE was designed to prepare for overloads, such as handling the worldwide uploading “spike” on New Year’s Eve (could be 3x video uploads).</li>
  <li>Building a scalable system is relevant only when the system is <strong>robust</strong>. When the system gets overloaded, it must <strong>gracefully degrade</strong>. It cannot crash and burn.</li>
  <li>Prepare for overload along two dimensions: at the pipeline level, and the task level.</li>
  <li>Pipeline level, when uploads overwhelm the system:
    <ul>
      <li>Do not cache original videos in upload: Preprocessor stops caching original videos. Workers then need fetch videos from the original storage, not from preprocessor. The cost here is disk latency is added to the critical path.</li>
      <li>Delay pipeline generation for incoming video. Distinguish the critical video pipeline requests and the non-critical ones, then delay the non-critical ones.</li>
      <li>Reroute traffic to a different (less busy) region (Asia, Europe, US west, etc.)</li>
    </ul>
  </li>
  <li>Task level (the tasks executed by <strong>workers</strong> in the pipeline), when too many tasks are running:
    <ul>
      <li>Push back non latency-sensitive jobs</li>
      <li>Turn off A/B tests, which try to figure out the best encoding for the given video</li>
      <li>Add more workers (this requires making it easy to add capacity to SVE)</li>
    </ul>
  </li>
</ul>

<h1 id="efficient">Efficient</h1>

<ul>
  <li>The high level problem statement here is: If we could use 100% CPU, how can we make the encoded video as small as possible?</li>
  <li>Find the optimal encoding settings to get the best balance between encoded video file size and time spent on encoding. The difficult part is modern encoders can have hundreds of settings for one video. Chance of picking optimal combination is extremely low.</li>
  <li>The adopted solution is:
    <ul>
      <li>Categorize each scene such as “minimal motion”, “rapid movement”, and “complex crowded scene”.</li>
      <li>Build a Neural Network Model and a large training data set to train the network.</li>
      <li>In SVE, video scene segments are sent to a Fingerprint generator, which generates fingerprints and sends them to the Neural Network Model.</li>
      <li>The neural network figures out optimal encoding settings (could be multiple) for each scene, and sends the encoding settings to encoders.</li>
      <li>the encoder takes the settings, and encodes the video scenes in multiple ways. Then discard the encoded videos which are below quality bar.</li>
    </ul>
  </li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170627-fb_02.png" width="600"></p>

<p>SVE achieved 20% smaller video file sizes. This is a huge saving of user’s data plans.</p>

<p>This Streaming Video Engine was designed, coded and tested in roughly 9 months. The most important learnings are:</p>

<ul>
  <li>E2E view: To find an optimal solution, we need look at the flow end to end</li>
  <li>Multi-dimensional flexibility is a key for making the system most useful</li>
  <li>Parallel and shadow mode testing to find correctness and scalability issues before production</li>
  <li>Design the ability to handle extreme products such as 360 videos</li>
  <li>Track direct measures (latency, reliability, etc.) and indirect measures (number of videos uploaded, watch times, etc.). Mapping indirect measures to direct measures could give you a good view in figuring out what you could do better next.</li>
</ul>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[How Instagram Moved to Python 3]]></title>
    <link href="http://euccas.github.io/blog/20170616/how-instagram-moved-to-python-3.html"/>
    <updated>2017-06-16T17:52:34-07:00</updated>
    <id>http://euccas.github.io/blog/20170616/how-instagram-moved-to-python-3</id>
    <content type="html"><![CDATA[<p>Instagram, the famous brunch sharing app, presented in <a href="https://us.pycon.org/2017/">PyCon 2017</a> and gave a talk in the keynote session on “How Instagram moves to Python 3”. If you have 15 minutes, read the interview with the speakers, Hui Ding and Lisa Guo from Instagram Infrastructure team, <a href="https://thenewstack.io/instagram-makes-smooth-move-python-3/]"><strong>here</strong></a>. If you have 45 minutes, watch their PyCon talk video, <a href="https://www.youtube.com/watch?v=66XoCk79kjM"><strong>here</strong></a>. If you have only 5 minutes, continue reading, <strong>right here</strong>.</p>

<p>Instagram’s backend, which serves over 400 million active users every day, is built on Python/Django stack. The decision on whether moving from Python 2 to Python 3, was really a decision on whether investing in a version of the language that was mature, but wasn’t going anywhere (Python 2 is expected to retire in 2020) – or the language that was the next version and had great and growing community support. The major motivations behind Instagram’s migration to Python 3 are:</p>

<ul>
  <li><strong>Typing support</strong> for dev velocity</li>
  <li>Better <strong>performance</strong> than Python 2</li>
  <li><strong>Community</strong> continues to make Python 3 better and faster</li>
</ul>

<p>The whole migration process took about 10 months, in roughly 3 stages.</p>

<!--more-->

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_00.png" width="520"></p>

<ul>
  <li>First off, the migration was done directly on the Master Branch, which means the developers were adding new features to the code while migration was ongoing. So in the beginning of the Mirgration process, infrastructure added support of Python 3 on the Master Branch to make the code be able to run with both Python 2 and Python 3 environment.</li>
  <li>Massive code modification for 3 months, with the help of Python package <a href="https://pypi.python.org/pypi/modernize"><strong>“modernize”</strong></a>. Meanwhile, upgraded Third-party packages to Python 3 (working rule: <em>No Python 3, no new package</em>). Also deleted unused, incompatible packages.</li>
  <li>Intensive unit testing for 2 months. One limitation is data compatibility issues typically do not show up in unit tests.</li>
  <li>Production rollout for another 4 months (push Python 3 to every developer’s sandbox)</li>
</ul>

<p>In the talk, Lisa shared the challenges they faced in the migration process and how did they solved those problems.</p>

<ul>
  <li>Differences in <strong>unicode</strong>, <strong>str</strong>, <strong>bytes</strong>. Solved by using helper functions.</li>
  <li><strong>Pickle memcache data format incompatibility</strong> in Python 2 and Python 3. Solved by isolating memcaches for Python 2 and Python 3.</li>
  <li><strong>Iterator</strong> differences, such as <code>map</code>. Solved by converting all maps to list in Python 3.</li>
  <li><strong>Dictionary order</strong> is different in different Python versions, which caused differences in the dumped JSON data. Solved by forcing <code>sorted_keys</code> in <code>json.dump</code> function.</li>
  <li>With Python 3, while CPU instructions per request decreased by 12%, max requests per second (capacity) had 0% increase! Found the root cause in the code of checking memory configuration, and the issue was memory optimization condition was never met in Python 3 as <code>True</code> because of unicode issue. Solved by adding a magical character <strong>“b”</strong>, just like this:</li>
</ul>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_01.png" width="520"></p>

<p>In Feb 2017, Instagram’s stack completely dropped Python 2 and moved to Python 3 (v3.6). So far they’ve got this from Python 3:</p>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_02.png" width="520"></p>

<p>One more thing, in the talk Hui Ding also briefly discussed a few <strong>Python Efficiency Strategies</strong> that Instagram used to support the growing number of features and users:</p>

<ul>
  <li>Build extensive tools to profile and understand perf bottleneck</li>
  <li>Proactively push stable, critical components to C/C++, e.g., memcached access library</li>
  <li>Use Cythonization to improve performance</li>
  <li>Future ideas: Make the Django stack completely Async? Create a new python runtime?</li>
</ul>

<p>Changing an existing service to use a new version of language can never be easy, especially when your service is at such a scale - serving millions of people. You just cannot afford to breaking the existing service. Moving to Python 3 in 10 months must be a challenging process. “It can be done. It worths it. Make it happen. And Make Python 3 better.”</p>

<p>Nice work Instagram!</p>

<p><img class="center" src="/images/post_images/2017/20170616-instagram_python3_03.png" width="520"></p>
]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[Levels of Design]]></title>
    <link href="http://euccas.github.io/blog/20161224/levels-of-design.html"/>
    <updated>2016-12-24T09:52:07-08:00</updated>
    <id>http://euccas.github.io/blog/20161224/levels-of-design</id>
    <content type="html"><![CDATA[<p>Recently I’m taking a course <a href="https://www.coursera.org/learn/algorithmic-toolbox/home/welcome">“Algorithm Toolbox”</a> on Coursera. This course provides me a good chance to review and enhance my knowledge in the fundamental algorithms, which usually would help on achieving better system design. This morning I came across one slide of this course and thought it could be very useful. Sharing it here.</p>

<p><img class="center" src="/images/post_images/2016/20161224-levels-of-design-1.png" width="600"></p>

<p>It’s important to keep the <strong>levels of (algorithm) design</strong> in mind when solving a problem.</p>

<!--more-->

<h1 id="level-1-naive-algorithm">Level 1: Naive Algorithm</h1>

<p>This is the solution that you can get just by taking the definition of a problem and turning it into an algorithm. This solution can solve the problem, but it is often very slow and inefficient.</p>

<p>The way I see the naive algorithm is it gives you something that works, and might be used to verify if alternative solutions are correct or not. But it’s important to not stay at the this solution. You should keep looking for better solutions.</p>

<h1 id="level-2-algorithm-by-way-of-standard-tools">Level 2: Algorithm by way of standard Tools</h1>

<p>You can look at the standard techniques and see if any of it applies to solving your problem. On this level, your goal is finding some standard techniques that work, often that don’t involve too much effort on your part, and give you something that work very well (better than the naive algorithm).</p>

<h1 id="level-3-optimized-algorithm">Level 3: Optimized Algorithm</h1>

<p>Remember there are always lots of ways to improve an existing solution. If you get a pretty good solution on level 2, why not taking one more step and see what you can do to improve it? Could you reduce the runtime from n-cubed to n-squared or n-squred to n? Could you come up with a shorter solution by rearranging the order or cut out some of the work? Could you use a data structure to speed things up? Think about all these possiblities and see if you can get a even better solution. (Often you will)</p>

<h1 id="level-4-magic-algorithm">Level 4: Magic Algorithm</h1>

<p>When the solutions from the previous three levels are not good enough, you’ll need some magic to get a better one. This can be hard. You will need some clever new ideas and unique insights of the problem you’re trying to solve. Even if you don’t get a magic solution in the end, the thought process will be beneficial.</p>

<p>Sometimes when I finish a project, I do have the feeling that the way I do it is just not good enough, even though the project has been proved to be successful, useful and solved a particular technical challenge we faced. Looking at the “Levels of Design”, I believe what I need to do is spending more efforts on the higher levels. Actually what you really need for getting a really good solution is not magic. What you need is mastering more existing good standard techniques, exploring the possibility for optimization, and a deep understanding of the problem you want to solve.</p>

]]></content>
  </entry>
  
  <entry>
    <title type="html"><![CDATA[GTAC: The Uber Challenge of Cross-Application Testing]]></title>
    <link href="http://euccas.github.io/blog/20160928/the-uber-challenge-of-cross-application-testinng.html"/>
    <updated>2016-09-28T23:17:30-07:00</updated>
    <id>http://euccas.github.io/blog/20160928/the-uber-challenge-of-cross-application-testinng</id>
    <content type="html"><![CDATA[<p>Inspired by Matt Cutts’ TED talk: <a href="https://www.ted.com/talks/matt_cutts_try_something_new_for_30_days?language=en">Try something new for 30 days</a>, I’m starting a “30 Days of GTAC” project. Google’s Test Automation Conference <a href="https://developers.google.com/google-test-automation-conference/">GTAC</a> is an annual test automation conference which brings together engineers from industry and academia to discuss advances in test automation and related engineering tools. In my “30 Days of GTAC” project, I’ll review the topics presented on GTAC. My goal is having a better and deeper understanding in modern testing technologies, methodologies, strategies, and practices.</p>

<p>Get it started! Day#1 topic is:</p>

<p><strong>The Uber Challenge of Cross-Application/Cross-Device Testing</strong></p>

<!--more-->

<ul>
  <li>Presenter: Apple Chow (Uber), Bian Jiang (Uber)</li>
  <li><a href="https://www.youtube.com/watch?v=p6gsssppeT0&amp;list=PLSIUOFhnxEiCWGsN9t5A-XOhRbmz54IS1&amp;index=3">Video</a></li>
  <li><a href="https://docs.google.com/presentation/d/1vYXhkvgLKun72Ix91LQDDWZQdcY5VOBqKVvI1Y6riYo/pub">Slides</a></li>
</ul>

<p><strong>My takeaways</strong></p>

<ul>
  <li>The challenge: End-to-end tests require cross application communication (between rider app and driver app)</li>
  <li>Uber’s solution: Octopus
    <ul>
      <li>Octopus coordinates communication across different apps running on different devices</li>
      <li>This solution can be adopted for any tests that require coordination/communication across different apps or devices</li>
    </ul>
  </li>
  <li>What makes testing Uber’s mobile apps significantly different from testing Google Maps?</li>
  <li>Why (built) Octopus? Unified (iOS and Android). Extensible (Integrate with different UI testing frameworks). Parallelized. Signaling (enabling cross-app and cross-device testing).</li>
  <li>What does Octopus do? Prepare test targets. Run tests (handles signaling). Create test results reports. Pull test artifacts. Perform clean ups. All from simple command line.</li>
  <li>Signaling between driver test and rider test: Conducted with Test Host to improve the consistency. API readSignal (blocking), writeSignal (nonblocking). Test hosts and test targets are connected via USB (reliable).</li>
  <li>Apple Chow wrote about Octopus on <a href="http://eng.uber.com/rescued-by-octopus/">Uber Engineering</a></li>
</ul>

]]></content>
  </entry>
  
</feed>
